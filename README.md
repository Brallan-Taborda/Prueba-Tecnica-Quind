<h1>Programa de generación de facturas de comisiones </h1>

<!-- TABLE OF CONTENTS -->
<details>
  <summary>Tabla de Contenido</summary>
  <ol>
    <li>
      <a href="#about-the-project">Acerca del proyecto </a>
      <ul>
        <li><a href="#built-with">Tecnologías implementadas</a></li>
      </ul>
    </li>
    <li>
      <a href="#getting-started">Configuraciones Iniciales</a>
      <ul>
        <li><a href="#prerequisites">Prerequisitos</a></li>
        <li><a href="#installation">Instalaciones</a></li>
      </ul>
    </li>   
    <li><a href="#contact">Autores</a></li>
  </ol>
</details>

<!-- ABOUT THE PROJECT -->
## Acerca del Proyecto

El siguiente proyecto implementa una aplicación escalable y modular para gestionar y ejecutar procesos de ETL (Extract, Transform, Load) utilizando **Python 3.12** como lenguaje de programación y **Apache Spark 3.5.5** como framework de procesamiento de datos. El objetivo es proporcionar una solución robusta, eficiente y observable para el manejo de grandes volúmenes de datos, siguiendo las mejores prácticas de programación orientada a objetos (POO) y principios SOLID.

## Tecnologías Implementadas

<div align="center">
    <code><img width="50" src="https://user-images.githubusercontent.com/25181517/192108374-8da61ba1-99ec-41d7-80b8-fb2f7c0a4948.png" alt="GitHub" title="GitHub"/></code>
    <code><img width="50" src="https://w7.pngwing.com/pngs/585/822/png-transparent-python-scalable-graphics-logo-javascript-creative-dimensional-code-angle-text-rectangle-thumbnail.png" alt="Python" title="Python"/></code>
    <code><img width="50" src="https://simpleicons.org/icons/apachespark.svg" alt="Apache Spark" title="Apache Spark"/></code>
    <code><img width="50" src="https://simpleicons.org/icons/java.svg" alt="Java" title="Java"/></code>
    <code><img width="50" src="https://simpleicons.org/icons/apachehadoop.svg" alt="Hadoop" title="Hadoop"/></code>
</div>

---

<!-- GETTING STARTED -->
## Configuraciones Iniciales

Para poner en funcionamiento una copia local, siga estos sencillos pasos de ejemplo.

### Prerrequisitos

Asegúrese de tener instalados los siguientes programas y herramientas:

1. **Java Development Kit (JDK)**  
   Se recomienda usar una versión compatible de Java (JDK 8, JDK 11 o JDK 17).  
   Descargue JDK desde: [https://adoptium.net/es/](https://adoptium.net/es/)

2. **Apache Spark 3.5.5**  
   Descargue la versión 3.5.5 de Spark desde:  
   [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)

3. **Apache Hadoop**  
   Hadoop es necesario para el manejo de sistemas de archivos distribuidos (HDFS).  
   Descargue Hadoop desde: [https://hadoop.apache.org/releases.html](https://hadoop.apache.org/releases.html)

4. **Python 3.12**  
   Asegúrese de tener Python 3.12 instalado.  
   Descargue Python desde: [https://www.python.org/downloads/](https://www.python.org/downloads/)

5. **Gestor de Dependencias (Opcional)**  
   Se recomienda usar un gestor de dependencias como `pip` o `poetry` para manejar las bibliotecas de Python.

---

### Instalaciones

Realizar las siguientes instalaciones necesarias.

1. Clonar el repositorio
   ```sh
   a
   ```
2. Abrir la terminal del sistema operativo en modo administrador e ingresar a la dirrección de la carpeta del repositorio
   ```sh
    a
   ```
3. Instalar las librerías necesarias con el siguiente comando
   ```sh
   pip install -r requirements.txt
   ```
4. Ejecutar el siguiente comando
   ```sh
   python main.py
   ```

## Desarrolladores

Brallan Taborda: [https://github.com/Brallan-Taborda](https://github.com/Brallan-Taborda)